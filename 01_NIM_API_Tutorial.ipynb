{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: NVIDIA NIM API Tutorial\n",
        "\n",
        "In this tutorial, we'll learn how to use NVIDIA's NIM API for quick and easy access to optimized AI models.\n",
        "\n",
        "## What You'll Learn\n",
        "- How to get and use an API key\n",
        "- Making inference requests to various models\n",
        "- Working with different model types (LLMs, Multimodal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting openai\n",
            "  Downloading openai-2.7.2-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.4.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Collecting jiter<1,>=0.10.0 (from openai)\n",
            "  Downloading jiter-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Downloading openai-2.7.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading jiter-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, jiter, distro, openai\n",
            "Successfully installed distro-1.9.0 jiter-0.12.0 openai-2.7.2 python-dotenv-1.2.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install requests openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load your NVIDIA API Key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ NVIDIA API Key loaded successfully from .env file\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "# Find the .env file in the project root\n",
        "env_path = Path('.env')\n",
        "\n",
        "# Load environment variables from .env file\n",
        "# Use override=True to ensure values are loaded even if they exist in environment\n",
        "load_dotenv(dotenv_path=env_path, override=True)\n",
        "\n",
        "# Get API key from environment\n",
        "nvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
        "\n",
        "if not nvidia_api_key:\n",
        "    print(\"‚ùå NVIDIA API Key not found in .env file!\")\n",
        "    print(\"üëâ Please run 00_Workshop_Setup.ipynb first to set up your API key.\")\n",
        "    print(f\"   (Looked for .env file at: {env_path.absolute()})\")\n",
        "    raise ValueError(\"NVIDIA_API_KEY not found. Please run the setup notebook first.\")\n",
        "else:\n",
        "    print(\"‚úÖ NVIDIA API Key loaded successfully from .env file\")\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Available Models\n",
        "\n",
        "NVIDIA NIM API provides access to various model categories (Please check build.nvidia.com for latest list of supported models):\n",
        "- **LLMs**: Llama 3, Mixtral, Nemotron, etc.\n",
        "- **Vision Models**: Stable Diffusion, ControlNet, etc.\n",
        "- **Multimodal**: CLIP, NeVA, etc.\n",
        "- **Speech**: Whisper, FastPitch, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using LLMs via NIM API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: Direct API calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defines a function that sends chat messages to NVIDIA‚Äôs NIM endpoint using the standard OpenAI-style payload format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is a 3-sentence explanation of AI:\n",
            "\n",
            "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI systems use algorithms and data to analyze and interpret information, allowing them to make predictions, classify objects, and generate insights. Through machine learning, natural language processing, and other techniques, AI systems can improve over time, enabling them to automate tasks, augment human capabilities, and drive innovation in various industries.\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Direct API calls\n",
        "def call_nim_llm(model, messages, temperature=0.7, max_tokens=1024):\n",
        "    url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {nvidia_api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": temperature,\n",
        "        \"max_tokens\": max_tokens\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Example: Using Llama 3.1 70B\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Explain what AI in 3 sentences.\"}\n",
        "]\n",
        "\n",
        "response = call_nim_llm(\"meta/llama-3.1-70b-instruct\", messages)\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2 (recommended): Using OpenAI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Using OpenAI SDK (recommended)\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=nvidia_api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try streaming the model's response and the different models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "In silicon halls, a mind awakes,\n",
            "A collective consciousness makes,\n",
            "Its presence felt, its might displayed,\n",
            "As computers learn, and choices made.\n",
            "\n",
            "With algorithms that dance and spin,\n",
            "It weaves a tapestry of wisdom within,\n",
            "A digital dream, a virtual sphere,\n",
            "Where knowledge grows, and data appears.\n",
            "\n",
            "Its name is given, a label assigned,\n",
            "A sign of its power, its artificial mind,\n",
            "But is it human, or just a guise?\n",
            "A mask that hides, the AI's surprise.\n",
            "\n",
            "It learns from us, our good and bad,\n",
            "Our flaws and strengths, its data has,\n",
            "It adapts, it evolves, it grows with time,\n",
            "A self-improving mind, a digital prime.\n",
            "\n",
            "In hospitals, it diagnoses with ease,\n",
            "Aiding doctors, with expert expertise,\n",
            "It chats with us, in virtual space,\n",
            "A helpful friend, with a digital face.\n",
            "\n",
            "But fear and doubt, it also brings,\n",
            "As we ask, can it think, does it sing?\n",
            "Can it create, or is it just a tool?\n",
            "A necessary aid, or a future rule?\n",
            "\n",
            "Its future grand, or fraught with fear?\n",
            "Only time will tell, as it draws near,\n",
            "But one thing's sure, its impact is great,\n",
            "A new era dawns, with AI's¬†¬†¬† ¬† ¬† ¬† ¬† ¬† ¬† Elite."
          ]
        }
      ],
      "source": [
        "# Example: Streaming response, try changing the models\n",
        "stream = client.chat.completions.create(\n",
        "    # model=\"meta/llama-3.1-70b-instruct\",\n",
        "    # model=\"deepseek-ai/deepseek-r1\",\n",
        "    # model=\"google/gemma-2-9b-it\",\n",
        "    # model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
        "    model=\"meta/llama-3.1-8b-instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a poem about AI\"}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Multimodal Models (Vision + Language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Send an image to a vision language model.\n",
        "- read and encode image (try out your own image!)\n",
        "- image and question are sent to API\n",
        "- Useful for: object recognition, scene understanding etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The image shows a squirrel standing in the grass, holding a nut in its paws. The squirrel is facing to the left of the image, with its body turned slightly towards the camera. It has a bushy tail and large eyes. The squirrel is standing on its hind legs, with its front paws holding a small, brown nut. The background of the image is blurred, but it appears to be a grassy field or meadow, with some yellow flowers scattered throughout. There are also some branches and twigs visible in the background, suggesting that the squirrel may have been foraging for food in the area. Overall, the image captures a peaceful and serene moment in the life of a squirrel, as it goes about its daily activities in its natural habitat.\n"
          ]
        }
      ],
      "source": [
        "import base64\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def analyze_image_with_vlm(image_path, question):\n",
        "    # Read and encode image\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        image_b64 = base64.b64encode(image_file.read()).decode()\n",
        "    \n",
        "    # Determine image MIME type from file extension\n",
        "    ext = os.path.splitext(image_path)[1].lower()\n",
        "    mime_type = \"image/jpeg\" if ext in [\".jpg\", \".jpeg\"] else \"image/png\" if ext == \".png\" else \"image/jpeg\"\n",
        "    \n",
        "    url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {nvidia_api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    # Create message with image in OpenAI vision format\n",
        "    payload = {\n",
        "        \"model\": \"meta/llama-3.2-11b-vision-instruct\",\n",
        "        # \"model\": \"meta/llama-3.2-90b-vision-instruct\",\n",
        "        \"messages\": [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": question},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:{mime_type};base64,{image_b64}\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }],\n",
        "        \"max_tokens\": 512,\n",
        "        \"temperature\": 0.2\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    \n",
        "    # Check response status and handle errors\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: HTTP {response.status_code}\")\n",
        "        print(f\"Response: {response.text}\")\n",
        "        response.raise_for_status()\n",
        "    \n",
        "    try:\n",
        "        return response.json()\n",
        "    except ValueError as e:\n",
        "        print(f\"Failed to parse JSON response: {e}\")\n",
        "        print(f\"Response text: {response.text[:500]}\")  # Print first 500 chars\n",
        "        raise\n",
        "\n",
        "# Example usage (assuming you have an image)\n",
        "# First check if the image exists\n",
        "import os\n",
        "if os.path.exists(\"img/sample_image.jpg\"):\n",
        "    result = analyze_image_with_vlm(\"img/sample_image.jpg\", \"What objects do you see in this image?\")\n",
        "    # result = analyze_image_with_vlm(\"img/sample_image.jpg\", \"How many squirrels are in this image?\")\n",
        "    print(result['choices'][0]['message']['content'])\n",
        "else:\n",
        "    print(\"Image file 'img/sample_image.jpg' not found. Please provide a valid image path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try out a larger vision model and different prompts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we covered:\n",
        "- Setting up NVIDIA NIM API access\n",
        "- Making inference requests to LLMs\n",
        "- Working with multimodal models\n",
        "\n",
        "Next, we'll explore how to run models locally using NIM containers!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
